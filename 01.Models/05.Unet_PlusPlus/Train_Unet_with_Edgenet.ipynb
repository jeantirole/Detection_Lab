{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RS_utils\n",
    "import RS_dataset\n",
    "import RS_models\n",
    "#---\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader\n",
    "#---\n",
    "import torch.nn as nn \n",
    "#---\n",
    "from lightning.fabric import Fabric\n",
    "import lightning as L\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "#-- data\n",
    "img_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/01.Models/04.SAM_fine/0.data/01.512_imgs\"\n",
    "mask_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/01.Models/04.SAM_fine/0.data/02.512_masks\"\n",
    "\n",
    "img_path_ship  = np.array(sorted(glob(os.path.join(img_path, \"*.png\"))) )\n",
    "mask_path_ship = np.array(sorted(glob(os.path.join(mask_path, \"*.png\"))) )\n",
    "\n",
    "aa = np.load(\"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/05.Training/Segmentation/03.data_list/512_ships.npy\")\n",
    "\n",
    "selected_paths_img = img_path_ship[aa]\n",
    "selected_paths_mask  = mask_path_ship[aa]\n",
    "\n",
    "\n",
    "#-- args\n",
    "TASK = \"SHIP\"\n",
    "MODEL_NAME = \"UNET_PP\"\n",
    "EXEC_VER = 35 \n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda:0\"\n",
    "DEVICES = [0,1,2,3]\n",
    "RESUME = False\n",
    "SAVE_EPOCH = 20\n",
    "\n",
    "\n",
    "#-- category \n",
    "ISAID_CLASSES_SHIP = (\n",
    "    'background','ship','harbor' \n",
    "    )\n",
    "ISAID_PALETTE_SHIP = {\n",
    "    0: (0, 0, 0), \n",
    "    1: (0, 0, 63), \n",
    "    2: (0, 100, 155)}\n",
    "\n",
    "#--- logger\n",
    "# Set up logging\n",
    "log_filename = datetime.datetime.now().strftime(f'./01.log/ver_{EXEC_VER}_%Y-%m-%d_%H-%M-%S.log')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "handler = logging.FileHandler(log_filename)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "#-- dataset\n",
    "train_dataset = RS_dataset.Seg_RS_dataset_ship(img_dir=selected_paths_img, mask_dir=selected_paths_mask, image_resize = None, phase=\"train\",palette=ISAID_PALETTE_SHIP )\n",
    "dataloader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "#--- model \n",
    "#model = RS_models.Edge_Net()\n",
    "model = smp.UnetPlusPlus(encoder_name=\"resnet152\",classes=3)\n",
    "#model = model.to(DEVICE)\n",
    "#criterion = nn.CrossEntropyLoss(reduction=\"mean\") \n",
    "#criterion = nn.BCELoss()\n",
    "criterion = RS_dataset.UNet_metric(num_classes=3)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "#-- resume\n",
    "if RESUME == True:\n",
    "    tgt_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/05.Training/Segmentation/02.ckpts\"\n",
    "    ckpt_path = os.path.join( tgt_path, sorted(os.listdir(tgt_path))[-1]  )\n",
    "    print(\"resume chekcpoint : \",ckpt_path)\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "#--- fabric setup \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        \n",
    "        # edge_net freeze\n",
    "        for param in self.model2.parameters():\n",
    "            param.requires_grad = False           \n",
    "\n",
    "    # def save_pretrained(self,path):\n",
    "    #     torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        # Save the model\n",
    "        self.save_pretrained(path)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, batch ):\n",
    "\n",
    "        #---------\n",
    "\n",
    "        \n",
    "        outputs = self.model1(**batch)\n",
    "            \n",
    "        pred = outputs.preds[:,:, 448:]\n",
    "        # resize pred => 512\n",
    "        pred = F.interpolate(pred,(512,512),mode='nearest')\n",
    "        pred = pred.float()\n",
    "        #print(\"pred.shape : \", pred.shape)\n",
    "        \n",
    "        #perceptual loss from edge_net\n",
    "        layer_1_out,layer_2_out,layer_3_out = self.model2(pred)\n",
    "        layer_1_gt ,layer_2_gt ,layer_3_gt  = self.model2(labels)\n",
    "        loss_1 = torch.nn.functional.l1_loss(layer_1_out, layer_1_gt)\n",
    "        loss_2 = torch.nn.functional.l1_loss(layer_2_out, layer_2_gt)\n",
    "        loss_3 = torch.nn.functional.l1_loss(layer_3_out, layer_3_gt)\n",
    "\n",
    "        #--- loss \n",
    "        loss_seg = outputs.loss\n",
    "        loss_percept = loss_1 + loss_2 + loss_3\n",
    "        \n",
    "        return loss_seg,loss_percept"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
