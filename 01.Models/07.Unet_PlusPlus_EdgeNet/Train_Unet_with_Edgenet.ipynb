{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import RS_utils\n",
    "import RS_dataset\n",
    "import RS_models\n",
    "#---\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import DataLoader\n",
    "#---\n",
    "import torch.nn as nn \n",
    "#---\n",
    "from lightning.fabric import Fabric\n",
    "import lightning as L\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#-- data\n",
    "img_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/01.Models/04.SAM_fine/0.data/01.512_imgs\"\n",
    "mask_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/01.Models/04.SAM_fine/0.data/02.512_masks\"\n",
    "\n",
    "img_path_ship  = np.array(sorted(glob(os.path.join(img_path, \"*.png\"))) )\n",
    "mask_path_ship = np.array(sorted(glob(os.path.join(mask_path, \"*.png\"))) )\n",
    "\n",
    "aa = np.load(\"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/05.Training/Segmentation/03.data_list/512_ships.npy\")\n",
    "\n",
    "selected_paths_img = img_path_ship[aa]\n",
    "selected_paths_mask  = mask_path_ship[aa]\n",
    "\n",
    "\n",
    "#-- args\n",
    "TASK = \"SHIP\"\n",
    "MODEL_NAME = \"UNET_PP_EDGE\"\n",
    "EXEC_VER = 40 \n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda:0\"\n",
    "DEVICES = [0,1,2,3]\n",
    "RESUME = False\n",
    "SAVE_EPOCH = 20\n",
    "\n",
    "\n",
    "#-- category \n",
    "ISAID_CLASSES_SHIP = (\n",
    "    'background','ship','harbor' \n",
    "    )\n",
    "ISAID_PALETTE_SHIP = {\n",
    "    0: (0, 0, 0), \n",
    "    1: (0, 0, 63), \n",
    "    2: (0, 100, 155)}\n",
    "\n",
    "#--- logger\n",
    "# Set up logging\n",
    "log_filename = datetime.datetime.now().strftime(f'./01.log/ver_{EXEC_VER}_%Y-%m-%d_%H-%M-%S.log')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "handler = logging.FileHandler(log_filename)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "#-- dataset\n",
    "train_dataset = RS_dataset.Seg_RS_dataset_SegEdge(img_dir=selected_paths_img, mask_dir=selected_paths_mask, image_resize = None, phase=\"train\",palette=ISAID_PALETTE_SHIP )\n",
    "dataloader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "#--- model \n",
    "#model = RS_models.Edge_Net()\n",
    "model_1 = smp.UnetPlusPlus(encoder_name=\"resnet152\",classes=3)\n",
    "tgt_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/01.Models/07.Unet_PlusPlus_EdgeNet/02.ckpts/ver_35_unet_epoch_101_iteration_470256.pt\"\n",
    "checkpoint = torch.load(tgt_path)\n",
    "model_1.load_state_dict(checkpoint)\n",
    "\n",
    "model_2 = RS_models.Edge_Net()\n",
    "tgt_path = \"/mnt/hdd/eric/.tmp_ipy/15.Lab_Detection/01.Models/07.Unet_PlusPlus_EdgeNet/02.ckpts/ver_31_edgenet_epoch_101.pt\"\n",
    "checkpoint = torch.load(tgt_path)\n",
    "model_2.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Edge_Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        \n",
    "        # seg_net train\n",
    "        for param in self.model1.parameters():\n",
    "            param.requires_grad = True       \n",
    "        \n",
    "        \n",
    "        # edge_net freeze\n",
    "        for param in self.model2.parameters():\n",
    "            param.requires_grad = False           \n",
    "\n",
    "    # def save_pretrained(self,path):\n",
    "    #     torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        # Save the model\n",
    "        self.save_pretrained(path)\n",
    "\n",
    "    \n",
    "    def forward(self, img,mask ):\n",
    "\n",
    "        #---------       \n",
    "        \n",
    "        outputs = self.model1(img)\n",
    "        \n",
    "        #perceptual loss from edge_net\n",
    "        layer_1_out,layer_2_out,layer_3_out = self.model2(outputs)\n",
    "        \n",
    "        layer_1_gt ,layer_2_gt ,layer_3_gt  = self.model2(mask)\n",
    "        \n",
    "        loss_1 = torch.nn.functional.l1_loss(layer_1_out, layer_1_gt,reduction='mean')\n",
    "        loss_2 = torch.nn.functional.l1_loss(layer_2_out, layer_2_gt,reduction='mean')\n",
    "        loss_3 = torch.nn.functional.l1_loss(layer_3_out, layer_3_gt,reduction='mean')\n",
    "\n",
    "        #--- loss \n",
    "        \n",
    "        loss_percept = loss_1 + loss_2 + loss_3\n",
    "        \n",
    "        return outputs,loss_percept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_net = CombinedModel(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask, edge = batch[0], batch[1],batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.eval()\n",
    "pred_1 = model_1(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = model_2(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = model_2(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_ , loss_percept = combine_net(img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.868900299072266"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_percept.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>only one element tensors can be converted to Python scalars\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0monly one element tensors can be converted to Python scalars\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs_[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "8.tmp.copied",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
