[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
INFO: You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
INFO:lightning.fabric.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
INFO: ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
INFO:lightning.pytorch.utilities.rank_zero:----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/convnext_xxlarge.clip_laion2b_soup_ft_in1k)
INFO:timm.models._hub:[timm/convnext_xxlarge.clip_laion2b_soup_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:RS_utils:{'input_size': (3, 256, 256), 'interpolation': 'bicubic', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'crop_pct': 1.0, 'crop_mode': 'center'}
INFO:RS_utils:Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=warn)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=tensor([0.4815, 0.4578, 0.4082]), std=tensor([0.2686, 0.2613, 0.2758]))
)
fabric
/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [3072, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [3072, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
INFO:RS_utils:{'epoch': 0, 'iteration': 0, 'progress': 0.0, 'loss': 2.6641, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 1, 'progress': 0.014492753623188406, 'loss': 14.105, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 2, 'progress': 0.028985507246376812, 'loss': 3.9261, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 3, 'progress': 0.043478260869565216, 'loss': 5.5925, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 4, 'progress': 0.057971014492753624, 'loss': 2.6598, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 5, 'progress': 0.07246376811594203, 'loss': 3.7059, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 6, 'progress': 0.08695652173913043, 'loss': 3.6711, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 7, 'progress': 0.10144927536231885, 'loss': 1.7959, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 8, 'progress': 0.11594202898550725, 'loss': 2.0526, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 9, 'progress': 0.13043478260869565, 'loss': 1.4883, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 10, 'progress': 0.14492753623188406, 'loss': 2.3803, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 11, 'progress': 0.15942028985507245, 'loss': 3.1879, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 12, 'progress': 0.17391304347826086, 'loss': 2.1427, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 13, 'progress': 0.18840579710144928, 'loss': 1.9379, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 14, 'progress': 0.2028985507246377, 'loss': 2.13, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 15, 'progress': 0.21739130434782608, 'loss': 2.0602, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 16, 'progress': 0.2318840579710145, 'loss': 2.0137, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 17, 'progress': 0.2463768115942029, 'loss': 2.4535, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 18, 'progress': 0.2608695652173913, 'loss': 2.0637, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 19, 'progress': 0.2753623188405797, 'loss': 1.8866, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 20, 'progress': 0.2898550724637681, 'loss': 2.0483, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 21, 'progress': 0.30434782608695654, 'loss': 2.0171, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 22, 'progress': 0.3188405797101449, 'loss': 1.5545, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 23, 'progress': 0.3333333333333333, 'loss': 2.3117, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 24, 'progress': 0.34782608695652173, 'loss': 1.8872, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 25, 'progress': 0.36231884057971014, 'loss': 2.3077, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 26, 'progress': 0.37681159420289856, 'loss': 1.9936, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 27, 'progress': 0.391304347826087, 'loss': 2.275, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 28, 'progress': 0.4057971014492754, 'loss': 1.4522, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 29, 'progress': 0.42028985507246375, 'loss': 1.7416, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 30, 'progress': 0.43478260869565216, 'loss': 1.7542, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 31, 'progress': 0.4492753623188406, 'loss': 2.0908, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 32, 'progress': 0.463768115942029, 'loss': 1.855, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 33, 'progress': 0.4782608695652174, 'loss': 2.2406, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 34, 'progress': 0.4927536231884058, 'loss': 1.693, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 35, 'progress': 0.5072463768115942, 'loss': 2.2077, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 36, 'progress': 0.5217391304347826, 'loss': 2.4219, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 37, 'progress': 0.5362318840579711, 'loss': 2.0438, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 38, 'progress': 0.5507246376811594, 'loss': 1.9071, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 39, 'progress': 0.5652173913043478, 'loss': 2.145, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 40, 'progress': 0.5797101449275363, 'loss': 2.3876, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 41, 'progress': 0.5942028985507246, 'loss': 2.1042, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 42, 'progress': 0.6086956521739131, 'loss': 2.4541, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 43, 'progress': 0.6231884057971014, 'loss': 2.134, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 44, 'progress': 0.6376811594202898, 'loss': 2.5224, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 45, 'progress': 0.6521739130434783, 'loss': 1.8915, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 46, 'progress': 0.6666666666666666, 'loss': 1.9299, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 47, 'progress': 0.6811594202898551, 'loss': 1.9171, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 48, 'progress': 0.6956521739130435, 'loss': 2.0593, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 49, 'progress': 0.7101449275362319, 'loss': 1.8117, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 50, 'progress': 0.7246376811594203, 'loss': 2.438, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 51, 'progress': 0.7391304347826086, 'loss': 1.9552, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 52, 'progress': 0.7536231884057971, 'loss': 1.9056, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 53, 'progress': 0.7681159420289855, 'loss': 1.9303, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 54, 'progress': 0.782608695652174, 'loss': 2.3055, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 55, 'progress': 0.7971014492753623, 'loss': 1.5421, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 56, 'progress': 0.8115942028985508, 'loss': 2.2769, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 57, 'progress': 0.8260869565217391, 'loss': 1.9717, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 58, 'progress': 0.8405797101449275, 'loss': 1.597, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 59, 'progress': 0.855072463768116, 'loss': 1.7395, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 60, 'progress': 0.8695652173913043, 'loss': 2.0258, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 61, 'progress': 0.8840579710144928, 'loss': 2.1947, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 62, 'progress': 0.8985507246376812, 'loss': 2.1383, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 63, 'progress': 0.9130434782608695, 'loss': 1.7323, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 64, 'progress': 0.927536231884058, 'loss': 2.1459, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 65, 'progress': 0.9420289855072463, 'loss': 1.7791, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 66, 'progress': 0.9565217391304348, 'loss': 1.4346, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 67, 'progress': 0.9710144927536232, 'loss': 1.9446, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 0, 'iteration': 68, 'progress': 0.9855072463768116, 'loss': 2.3524, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 0, 'progress': 0.0, 'loss': 2.3319, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 1, 'progress': 0.014492753623188406, 'loss': 2.2634, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 2, 'progress': 0.028985507246376812, 'loss': 1.3239, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 3, 'progress': 0.043478260869565216, 'loss': 2.7058, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 4, 'progress': 0.057971014492753624, 'loss': 1.9894, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 5, 'progress': 0.07246376811594203, 'loss': 1.8575, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 6, 'progress': 0.08695652173913043, 'loss': 1.7041, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 7, 'progress': 0.10144927536231885, 'loss': 2.6937, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 8, 'progress': 0.11594202898550725, 'loss': 2.3772, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 9, 'progress': 0.13043478260869565, 'loss': 1.4565, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 10, 'progress': 0.14492753623188406, 'loss': 2.04, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 11, 'progress': 0.15942028985507245, 'loss': 2.1557, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 12, 'progress': 0.17391304347826086, 'loss': 1.6184, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 13, 'progress': 0.18840579710144928, 'loss': 1.8169, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 14, 'progress': 0.2028985507246377, 'loss': 1.5817, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 15, 'progress': 0.21739130434782608, 'loss': 1.7512, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 16, 'progress': 0.2318840579710145, 'loss': 2.3301, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 17, 'progress': 0.2463768115942029, 'loss': 1.6055, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 18, 'progress': 0.2608695652173913, 'loss': 1.9978, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 19, 'progress': 0.2753623188405797, 'loss': 1.9838, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 20, 'progress': 0.2898550724637681, 'loss': 1.8384, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 21, 'progress': 0.30434782608695654, 'loss': 1.9352, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 22, 'progress': 0.3188405797101449, 'loss': 1.8933, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 23, 'progress': 0.3333333333333333, 'loss': 2.4525, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 24, 'progress': 0.34782608695652173, 'loss': 1.6748, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 25, 'progress': 0.36231884057971014, 'loss': 1.5892, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 26, 'progress': 0.37681159420289856, 'loss': 2.0713, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 27, 'progress': 0.391304347826087, 'loss': 2.042, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 28, 'progress': 0.4057971014492754, 'loss': 2.0145, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 29, 'progress': 0.42028985507246375, 'loss': 1.7118, 'batch_size': 4, 'lr': 0.00025}
INFO:RS_utils:{'epoch': 1, 'iteration': 30, 'progress': 0.43478260869565216, 'loss': 1.8239, 'batch_size': 4, 'lr': 0.00025}
Traceback (most recent call last):
  File "01.finetune.py", line 128, in <module>
    scaler.step(optimizer)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 340, in step
    return optimizer.step(*args, **kwargs)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/lightning/fabric/wrappers.py", line 92, in step
    output = self._strategy.optimizer_step(
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/lightning/fabric/strategies/strategy.py", line 203, in optimizer_step
    return self.precision.optimizer_step(optimizer, **kwargs)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/lightning/fabric/plugins/precision/precision.py", line 124, in optimizer_step
    return optimizer.step(**kwargs)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/optim/adamw.py", line 184, in step
    adamw(
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/optim/adamw.py", line 335, in adamw
    func(
  File "/mnt/hdd/eric/.conda/envs/mapv2/lib/python3.8/site-packages/torch/optim/adamw.py", line 543, in _multi_tensor_adamw
    torch._foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, 1 - beta2)
KeyboardInterrupt