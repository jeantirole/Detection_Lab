{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy with torch \n",
    "\n",
    "Case 구현 및 검증 \n",
    "\n",
    "- case 1 : data array with 1 dimension \n",
    "- case 2 : data array with n dimension \n",
    "- case 3 : (h,w) data array with (d) dimension eg. image data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example of target with class indices\\nloss = nn.CrossEntropyLoss()\\ninput = torch.randn(3, 5, requires_grad=True)\\ntarget = torch.empty(3, dtype=torch.long).random_(5)\\noutput = loss(input, target)\\noutput.backward()\\n\\n# Example of target with class probabilities\\ninput = torch.randn(3, 5, requires_grad=True)\\ntarget = torch.randn(3, 5).softmax(dim=1)\\noutput = loss(input, target)\\noutput.backward()\\n\\n\\npytorch document example 이다. \\n\\n두 예제의 가장 큰 차이점은 target 의 유형 (indices or class probabilities) 이다. \\n따라서 indices(long) => class probabilities(one-hot) 으로 변환을 해서 쓰거나 혹은 쓰지 않아도 된다. \\nsegmentation 의 경우에, iou or dice 를 계산해주어야 할 일이 많으므로, one-hot encoded 를 matirx 사이에 넣어주는 경우도 흔하다. \\n\\nsmall tips for usage : \\n\\n- torch 의 CE loss 는, softmax 내장이기 때문에, 모델의 마지막 layer 가 softmax 일 필요는 없다. \\n- LogSoftmax + NLLLoss combined into one function => torch.nn.functional.CrossEntropy  \\n- (e.g. [2, 3, 1, 1] vs [[0 0 1 0], [0 0 0 1], [0 1 0 0], [0 1 0 0] for CrossEntropyLoss and BCELoss) \\n\\n\\n- 단, BCE 를 사용할 경우에, softmax 를 따로 적용해 주어야 한다. \\n  혹은 BCEWithLogitsLoss 를 사용하면 된다. \\n\\n- nn.CrossEntropyLoss 의 weight 를 잡아주면은, focal_loss 처럼 활용가능하다\\n- ref : https://medium.com/unpackai/cross-entropy-loss-in-ml-d9f22fc11fe0\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    ">>> # Example of target with class indices\n",
    ">>> loss = nn.CrossEntropyLoss()\n",
    ">>> input = torch.randn(3, 5, requires_grad=True)\n",
    ">>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
    ">>> output = loss(input, target)\n",
    ">>> output.backward()\n",
    ">>>\n",
    ">>> # Example of target with class probabilities\n",
    ">>> input = torch.randn(3, 5, requires_grad=True)\n",
    ">>> target = torch.randn(3, 5).softmax(dim=1)\n",
    ">>> output = loss(input, target)\n",
    ">>> output.backward()\n",
    "\n",
    "\n",
    "pytorch document example 이다. \n",
    "\n",
    "두 예제의 가장 큰 차이점은 target 의 유형 (indices or class probabilities) 이다. \n",
    "따라서 indices(long) => class probabilities(one-hot) 으로 변환을 해서 쓰거나 혹은 쓰지 않아도 된다. \n",
    "segmentation 의 경우에, iou or dice 를 계산해주어야 할 일이 많으므로, one-hot encoded 를 matirx 사이에 넣어주는 경우도 흔하다. \n",
    "\n",
    "small tips for usage : \n",
    "\n",
    "- torch 의 CE loss 는, softmax 내장이기 때문에, 모델의 마지막 layer 가 softmax 일 필요는 없다. \n",
    "- LogSoftmax + NLLLoss combined into one function => torch.nn.functional.CrossEntropy  \n",
    "- (e.g. [2, 3, 1, 1] vs [[0 0 1 0], [0 0 0 1], [0 1 0 0], [0 1 0 0] for CrossEntropyLoss and BCELoss) \n",
    "\n",
    "\n",
    "- 단, BCE 를 사용할 경우에, softmax 를 따로 적용해 주어야 한다. \n",
    "  혹은 BCEWithLogitsLoss 를 사용하면 된다. \n",
    "\n",
    "- nn.CrossEntropyLoss 의 weight 를 잡아주면은, focal_loss 처럼 활용가능하다\n",
    "- ref : https://medium.com/unpackai/cross-entropy-loss-in-ml-d9f22fc11fe0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import torch.nn as nn\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0.], dtype=torch.float64)\n",
      "1.7782022953033447\n"
     ]
    }
   ],
   "source": [
    "# Case 1 ------------------------------------------------\n",
    "# conditions) \n",
    "# target 이 indices\n",
    "# 데이터 dimension 이 1 \n",
    " \n",
    "input = torch.tensor([-0.7,0.2,0.2])\n",
    "target = torch.tensor([1,0,0], dtype=float)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "output = loss(input, target)\n",
    "print(target)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_implementation(input, target, indice):\n",
    "    \n",
    "    input = torch.softmax(input, dim=-1)\n",
    "    if indice == True:\n",
    "        target = F.one_hot(target, num_classes=5)\n",
    "    \n",
    "    CE_res = -1 * torch.sum( (target * torch.log(input)) )\n",
    "    return CE_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7782, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "output_ = ce_implementation(input, target, indice=False)\n",
    "print(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 0])\n",
      "1.5616998672485352\n"
     ]
    }
   ],
   "source": [
    "# Case 2 ------------------------------------------------\n",
    "# conditions) \n",
    "# target 이 indices\n",
    "# 데이터 dimension 이 n (>2)\n",
    " \n",
    "input = torch.tensor([[0.7,0.2,0.2,0.1],\n",
    "                      [0.2,0.7,0.2,0.1],\n",
    "                      [0.2,0.2,0.8,0.1]\n",
    "                      ])\n",
    "target = torch.tensor([2,3,0], dtype=torch.long)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "output = loss(input, target)\n",
    "print(target)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_imple_n_dimension(input, target):\n",
    "\n",
    "    input_ = torch.softmax(input,dim=-1)\n",
    "    target_ = F.one_hot(target)\n",
    "    r = -1 * torch.sum( target_ * torch.log(input_) ) / input_.shape[0] \n",
    "    return r  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ = ce_imple_n_dimension(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5617)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# case 3\n",
    "- 2d array with n dimension \n",
    "- 2d array with N categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npred.shape\\ntorch.Size([1, 5, 20, 20])\\n\\ntarget.shape\\ntorch.Size([1, 20, 20])\\n\\ntarget shape 에서, (1,1,20,20) 이렇게 맞춰주지 말고 \\n1 channel 을 무시해줘야 한다. \\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 3 ------------------------------------------------\n",
    "# conditions) \n",
    "# target 이 indices\n",
    "# 데이터 dimension 이 n (>2)\n",
    "n_class = 2\n",
    "pred = torch.randn(4,n_class,20,20)\n",
    "target = torch.randint(0,n_class,(4,20,20))\n",
    "# => 여기에서, \n",
    "'''\n",
    "pred.shape\n",
    "torch.Size([1, 5, 20, 20])\n",
    "\n",
    "target.shape\n",
    "torch.Size([1, 20, 20])\n",
    "\n",
    "target shape 에서, (1,1,20,20) 이렇게 맞춰주지 말고 \n",
    "1 channel 을 무시해줘야 한다. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 20, 20])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 20])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "output = loss(pred, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 20, 20])\n",
      "torch.Size([4, 20, 20, 5])\n"
     ]
    }
   ],
   "source": [
    "# 까지꺼 한번 해보자 ㅋㅋ \n",
    "\n",
    "pred1 = torch.softmax(pred, dim=1)\n",
    "print(pred1.shape)\n",
    "pred2 = pred1.permute(0,2,3,1)\n",
    "print(pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 20, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 20, 5])\n"
     ]
    }
   ],
   "source": [
    "target1 = F.one_hot(target)\n",
    "print(target1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9370)\n"
     ]
    }
   ],
   "source": [
    "r = -1 * torch.sum( target1 * torch.log(pred2) ) / (20*20*4)\n",
    "print(r) \n",
    "\n",
    "# 위의 cross entropy 값과 같음 검증 완료 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4\n",
    "\n",
    "- BCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npred.shape\\ntorch.Size([1, 5, 20, 20])\\n\\ntarget.shape\\ntorch.Size([1, 20, 20])\\n\\ntarget shape 에서, (1,1,20,20) 이렇게 맞춰주지 말고 \\n1 channel 을 무시해줘야 한다. \\n\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 4 ------------------------------------------------\n",
    "# conditions) \n",
    "# target 이 indices\n",
    "# 데이터 dimension 이 n (>2)\n",
    "n_class = 1\n",
    "pred = torch.randn(4,n_class,20,20)\n",
    "target = torch.randint(0,n_class+1,(4,1,20,20))\n",
    "# => 여기에서, \n",
    "'''\n",
    "pred.shape\n",
    "torch.Size([1, 5, 20, 20])\n",
    "\n",
    "target.shape\n",
    "torch.Size([1, 20, 20])\n",
    "\n",
    "target shape 에서, (1,1,20,20) 이렇게 맞춰주지 말고 \n",
    "1 channel 을 무시해줘야 한다. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 20, 20])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 20, 20])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/eric/.conda/envs/8.tmp.copied/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "sig_pred = nn.functional.sigmoid(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8060)\n"
     ]
    }
   ],
   "source": [
    "pred = pred.float()\n",
    "target = target.float()\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "loss_ = bce_loss(sig_pred,target)\n",
    "print(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7831)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_metric():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\") # \"mean\" or \"sum\"\n",
    "\n",
    "    def __call__(self, pred, target):\n",
    "        # cross-entropy\n",
    "        loss1 = self.CE_loss(pred, target)\n",
    "        \n",
    "        # dice-coefficient\n",
    "        onehot_pred = F.one_hot(torch.argmax(pred, dim=1), num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
    "        onehot_target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
    "        loss2 = self._get_dice_loss(onehot_pred, onehot_target)\n",
    "        \n",
    "        # total loss\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # dice score\n",
    "        dice_coefficient = self._get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
    "        return loss, dice_coefficient\n",
    "\n",
    "    def _get_dice_coeffient(self, pred, target):\n",
    "        set_inter = torch.dot(pred.reshape(-1).float(), target.reshape(-1).float())\n",
    "        set_sum = pred.sum() + target.sum()\n",
    "        if set_sum.item() == 0:\n",
    "            set_sum = 2 * set_inter\n",
    "        dice_coeff = (2 * set_inter) / (set_sum + 1e-9)\n",
    "        return dice_coeff\n",
    "\n",
    "    def _get_multiclass_dice_coefficient(self, pred, target):\n",
    "        dice = 0\n",
    "        for class_index in range(1, self.num_classes):\n",
    "            dice += self._get_dice_coeffient(pred[class_index], target[class_index])\n",
    "        return dice / (self.num_classes - 1)\n",
    "\n",
    "    def _get_batch_dice_coefficient(self, pred, target):\n",
    "        num_batch = pred.shape[0]\n",
    "        dice = 0\n",
    "        for batch_index in range(num_batch):\n",
    "            dice += self._get_multiclass_dice_coefficient(pred[batch_index], target[batch_index])\n",
    "        return dice / num_batch\n",
    "\n",
    "    def _get_dice_loss(self, pred, target):\n",
    "        return 1 - self._get_batch_dice_coefficient(pred, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "8.tmp.copied",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
